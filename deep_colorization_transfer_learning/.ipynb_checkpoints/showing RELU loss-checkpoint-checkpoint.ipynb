{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from shutil import copy2\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "import re\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting the default data type to float 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 : Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of given list: 750\n",
      "number of images: 750\n",
      "Training Set Size: 675\n",
      "Testing Set Size: 75\n",
      "675\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"C:/Users/aashish/Desktop/DLCG2/face_images/\"\n",
    "files = glob.glob(\"C:/Users/aashish/Desktop/DLCG2/face_images/*.jpg\")\n",
    "\n",
    "\n",
    "print(\"Length of given list:\", len(files))\n",
    "train_data = \"C:/Users/aashish/Desktop/DLCG2/train/tensor/\"\n",
    "test_data = \"C:/Users/aashish/Desktop/DLCG2/test/tensor/\"\n",
    "\n",
    "os.makedirs(train_data, exist_ok = True)\n",
    "os.makedirs(test_data, exist_ok = True)\n",
    "\n",
    "num_images = len(next(os.walk(img_dir))[2])\n",
    "print(\"number of images:\", num_images)\n",
    "\n",
    "for i, file in enumerate(os.listdir(img_dir)):\n",
    "    if i < (0.1*num_images):\n",
    "        copy2(img_dir + file, test_data + file)\n",
    "        continue\n",
    "    else:\n",
    "        copy2(img_dir + file, train_data + file)\n",
    "\n",
    "print(\"Training Set Size:\", len(next(os.walk(train_data))[2]))\n",
    "print(\"Testing Set Size:\", len(next(os.walk(test_data))[2]))\n",
    "\n",
    "\n",
    "training_data = glob.glob('C:/Users/aashish/Desktop/DLCG2/train/tensor/*.jpg')\n",
    "testing_data = glob.glob('C:/Users/aashish/Desktop/DLCG2/test/tensor/*.jpg')\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "# # for f1 in files:\n",
    "# #     img = cv.imread(f1)\n",
    "# #     img = cv.cvtColor(img, cv.COLOR_BGR2RGB) #converting to RGB\n",
    "# #     data.append(img)\n",
    "# # data_tensor = torch.tensor(data).permute(0,3,1,2) #Converting nparray to tensor and in desired order\n",
    "# # print (img.shape)\n",
    "# print (data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class to return augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedImageDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self,index):\n",
    "        global channel_a, channel_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        original_image = np.asarray(img)\n",
    "        \n",
    "        img_lab = rgb2lab(original_image)\n",
    "        img_lab = img_lab + 128\n",
    "        img_lab = img_lab / 255\n",
    "        \n",
    "        channel_a = img_lab[:, :, 1:2]\n",
    "        channel_a = torch.from_numpy(channel_a.transpose((2,0,1))).float()\n",
    "        \n",
    "        channel_b = img_lab[:, :, 2:3]\n",
    "        channel_b = torch.from_numpy(channel_b.transpose((2,0,1))).float()\n",
    "        \n",
    "        img_gray = rgb2gray(original_image)\n",
    "        img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
    "        \n",
    "        return channel_a, channel_b, img_gray\n",
    "    \n",
    "    \n",
    "class AugmentedImageDataset_RELU(datasets.ImageFolder):\n",
    "    def __getitem__(self,index):\n",
    "        global channel_a, channel_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self. transform(img)\n",
    "        original_image = np.asarray(img)\n",
    "        \n",
    "        img_lab = rgb2lab(original_image)\n",
    "        channel_a = img_lab[:, :, 1:2]\n",
    "        channel_a = torch.from_numpy(channel_a.transpose((2,0,1))).float()\n",
    "        \n",
    "        channel_b = img_lab[:, :, 2:3]\n",
    "        channel_b = torch.from_numpy(channel_b.transpose((2,0,1))).float()\n",
    "        \n",
    "        img_gray = rgb2gray(original_image)\n",
    "        img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
    "        \n",
    "        return channel_a, channel_b, img_gray\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### augmenting the dataset and loading into 10* tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(128),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.RandomResizedCrop(128),\n",
    "#                                 transforms.ToTensor(), \n",
    "#                                        transforms.Normalize([0.6, 0.6, 0.6], [0.6, 0.6, 0.6])\n",
    "                               ])\n",
    "\n",
    "train_dataset =[]\n",
    "train_dataset.append(AugmentedImageDataset_RELU('C:/Users/aashish/Desktop/DLCG2/train'))\n",
    "for i in range(9):\n",
    "    train_dataset.append(AugmentedImageDataset_RELU('C:/Users/aashish/Desktop/DLCG2/train', transform))\n",
    "    \n",
    "augmented_dataset = ConcatDataset(train_dataset)\n",
    "print(len(augmented_dataset))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading all the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = dict(shuffle = True, batch_size = 32)\n",
    "# if cuda:\n",
    "#     train_args = dict(shuffle = True, batch_size = 8, num_workers = 1, pin_memory = True)\n",
    "    \n",
    "augmented_batch_train = DataLoader(dataset = augmented_dataset, **train_args)\n",
    "augmented_batch_test = DataLoader(dataset = AugmentedImageDataset('C:/Users/aashish/Desktop/DLCG2/test')) \n",
    "#not applying transforms on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available()else  \"cpu\")\n",
    "is_cuda_available = True if torch.cuda.is_available() else False\n",
    "if is_cuda_available:\n",
    "    num_workers = 8\n",
    "else:\n",
    "    num_workers = 0\n",
    "# print(device)\n",
    "print(format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel=1, hidden_channel=3, out_dims=2, train_mode = \"regressor\"):\n",
    "        super(Regressor,self).__init__()\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channel, out_channels = 32, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels =128, out_channels = 256, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        if self.train_mode == \"regressor\":\n",
    "            self.lin = nn.Linear(in_features=512 * 2 * 2, out_features=out_dims)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        if self.train_mode == \"regressor\":\n",
    "            y = torch.sigmoid(self.lin(features.reshape(-1,512*2*2)))\n",
    "            return y\n",
    "        else:\n",
    "            return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer(nn.Module):\n",
    "    def __init__(self, in_channel=3, hidden_channel=3, out_channel=2,activation_function = \"sigmoid\"):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        self.features = Regressor(in_channel=1, hidden_channel = 3, out_dims=2, train_mode=\"colorizer\")\n",
    "        \n",
    "        self.up_sampling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels = 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels = 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels = 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels = 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels = 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels = out_channel, kernel_size=4, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return torch.sigmoid(self.up_sampling(self.features(x)))\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return torch.tanh(self.up_sampling(self.features(x)))\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return torch.relu(self.up_sampling(self.features(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ab_mean(a_channel, b_channel):\n",
    "    a_channel_mean = a_channel.mean(dim = (2,3))\n",
    "    b_channel_mean = b_channel.mean(dim=(2,3))\n",
    "    a_b_mean = torch.cat([a_channel_mean,b_channel_mean], dim = 1)\n",
    "    \n",
    "    return a_b_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(gray_input, ab_input, save_path = None, save_name = None, device = \"cpu\"):\n",
    "    plt.clf()\n",
    "    color_image = torch.cat((gray_input, ab_input), 0).numpy()\n",
    "    color_image = color_image.transpose((1,2,0))\n",
    "    \n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
    "    color_image = lab2rgb(color_image.astype(np.float64))\n",
    "    gray_input = gray_input.squeeze().numpy()\n",
    "    if save_path is not None and save_name is not None:\n",
    "        plt.imsave(arr=gray_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap = 'gray')\n",
    "        plt.imsave(arr=color_image, fname = '{}{}'.format(save_path['colorized'], save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(gray, orig, new, fig_name):\n",
    "    plt.clf()\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1, 3, 1)\n",
    "    plt.imshow(mpimg.imread(gray))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 2)\n",
    "    plt.imshow(mpimg.imread(orig))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 3)\n",
    "    plt.imshow(mpimg.imread(new))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.draw()\n",
    "    plt.savefig(fig_name, dpi=220)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping_DCN:\n",
    "    def __init__(self, patience = 7, verbose = False, delta = 0, model_path = None, trace_func = print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.model_path = model_path\n",
    "        self.trace_func = trace_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hyperparameters():\n",
    "    parameters = dict(lr=[0.001],\n",
    "                     weight_decay=[1e-5],\n",
    "                     epoch=[100])\n",
    "    hyperparam = [i for i in parameters.values()]\n",
    "    return hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg:\n",
    "    def reg_train(self, augmented_dataset_batch, device):\n",
    "        print(\"...Regressor training Started...\")\n",
    "        model = Regressor(in_channel = 1, hidden_channel = 3, out_dims = 2, train_mode = \"regressor\").to(device)\n",
    "\n",
    "        lossF = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-4)\n",
    "\n",
    "        loss_train = []\n",
    "\n",
    "        #start training\n",
    "        epochs = 100\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "\n",
    "            for batch in augmented_dataset_batch:\n",
    "                l_channel, a_channel, b_channel = batch\n",
    "                l_channel = l_channel.to(device)\n",
    "\n",
    "                a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "                a_b_mean_hat = model(l_channel)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    loss = lossF(a_b_mean_hat.float().cuda(),a_b_mean.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss = Lossf(a_b_mean_hat.float(), a_b_mean.float()).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"epoch: {0}, loss: {1}\".format(epoch, total_loss))\n",
    "            loss_train.append(total_loss)\n",
    "\n",
    "        # plotting loss/ epoch graph\n",
    "        plt.ion()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_train)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        plt.draw()\n",
    "        plt.savefig('C:/Users/aashish/Desktop/DLCG2/Graphs', dpi = 200)\n",
    "        plt.clf()\n",
    "        torch.save(model.state_dict(), \"C:/Users/aashish/Desktop/DLCG2/Graphs/Reg.pth\")\n",
    "        \n",
    "    def reg_test(self, augmented_dataset_batch, device):\n",
    "        print(\"<<Regressor testing started>>\")\n",
    "        model = Regressor(in_channel= 1, hidden_channel=3, out_dims = 2, train_mode = \"regressor\").to(device)\n",
    "        model.load_state_dict(torch.load(\"C:/Users/aashish/Desktop/DLCG2/Graphs/Reg.pth\", map_location=device))\n",
    "        \n",
    "        a_list = []\n",
    "        b_list = []\n",
    "        lossF = nn.MSELoss()\n",
    "        total_loss = 0\n",
    "        loss_test = []\n",
    "        for batch in augmented_dataset_batch:\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "            \n",
    "            a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "            a_b_mean_hat = model(l_channel).detach()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_mean_hat.float().cuda(), a_b_mean.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_mean_hat.float(), a_b_mean.float()).to(device)\n",
    "            loss_test.append(loss.item())\n",
    "            \n",
    "            a_b_pred = a_b_mean_hat[0].cpu().numpy()\n",
    "            a_list.append(a_b_pred[0])\n",
    "            b_list.append(a_b_pred[1])\n",
    "        print(\"MSE:\", np.average(np.asarray(loss_test)))\n",
    "        print(\"Num_Image || Mean a || Mean b\")\n",
    "        for i in range(1, len(a_list)):\n",
    "            print(\"Image:{0} mean_a: {1} mean_b:{2}\".format(i, (a_list[i] * 255)-128, (b_list[i]*255)-128))\n",
    "            \n",
    "    def train_colorizer(self, augmented_dataset_batch, activation_function, model_name, device):\n",
    "        print(\"...Activation Function...\", activation_function)\n",
    "        print(\"...colorizer training started...\")\n",
    "        \n",
    "        \n",
    "        parameters = Hyperparameters()\n",
    "        for lr, weight_decay, epoch in product(*parameters):\n",
    "            print(\"Epoch: {0}, lr: {1}, weight decay:{2}\".format(epoch, lr, weight_decay))\n",
    "            model = Colorizer(in_channel=3, hidden_channel=3, activation_function = activation_function).to(device)\n",
    "        \n",
    "            lossF = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr =lr, weight_decay = weight_decay)\n",
    "#             saved_model_path = model_name.format(epoch, lr, weight_decay)\n",
    "            loss_train = []\n",
    "            early_stopping = EarlyStopping_DCN(patience = 50, verbose=True, model_path=\"C:/Users/aashish/Desktop/DLCG2/Graphs/color.pth\")\n",
    "#             epochs = 100\n",
    "            for epoch in range(epoch):\n",
    "                total_train_loss = 0\n",
    "                total_val_loss = 0\n",
    "                model.train()\n",
    "                \n",
    "                for batch in augmented_dataset_batch:\n",
    "                    channel_l, channel_a, channel_b = batch\n",
    "                    channel_l = channel_l.to(device)\n",
    "                    \n",
    "                    channel_a_b = torch.cat([channel_a, channel_b], dim = 1)\n",
    "                    channel_a_b_hat = model(channel_l)\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        loss = lossF(channel_a_b_hat.float().cuda(), channel_a_b.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss = lossF(channel_a_b_hat.float(), channel_a_b.float()).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_train_loss += loss.item()\n",
    "                    \n",
    "                print(\"epoch:{0}, loss:{1}\".format(epoch, total_train_loss))\n",
    "                loss_train.append(total_train_loss)\n",
    "                \n",
    "                \n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early Stop\")\n",
    "                    break\n",
    "                    \n",
    "                # plotting loss/ epoch graph\n",
    "            plt.ion()\n",
    "            fig = plt.figure()\n",
    "            plt.plot(loss_train)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.show()\n",
    "            plt.draw()\n",
    "            plt.savefig('C:/Users/aashish/Desktop/DLCG2/Graphs/loss_plot_path.jpeg', dpi = 220)\n",
    "            plt.clf()\n",
    "            torch.save(model.state_dict(), \"C:/Users/aashish/Desktop/DLCG2/Graphs/color.pth\")\n",
    "        \n",
    "        \n",
    "    def test_colorizer(self, augmented_dataset_batch, activation_function, save_path, model_name, device):\n",
    "        parameters = Hyperparameters()\n",
    "        for lr, weight_decay, epoch in product(*parameters):\n",
    "#             print(\"------\")\n",
    "            print(\"Epoch : {0}, lr: {1}, Weight_decay: {2}\".format(epoch, lr, weight_decay))\n",
    "            \n",
    "#             saved_model_path = model_name.format(epoch, lr, weight_decay)\n",
    "                \n",
    "            print(activation_function)\n",
    "            print(\"--- Colorizer Testing Started ---\")\n",
    "            model = Colorizer(in_channel=3, hidden_channel = 3, activation_function = activation_function).to(device)\n",
    "            model.load_state_dict(torch.load(\"C:/Users/aashish/Desktop/DLCG2/Graphs/color.pth\", map_location = device))\n",
    "                \n",
    "            lossF = nn.MSELoss()\n",
    "            num = 0\n",
    "            for batch in augmented_dataset_batch:\n",
    "                num += 1\n",
    "                channel_l, channel_a, channel_b = batch\n",
    "                channel_l = channel_l.to(device)\n",
    "                    \n",
    "                channel_a_b = torch.cat([channel_a, channel_b], dim=1)\n",
    "                channel_a_b_hat = model(channel_l).detach()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    loss = lossF(channel_a_b_hat.float().cuda(), channel_a_b.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss = lossF(channel_a_b_hat.float(), channel_a_b.float()).to(device)\n",
    "                    \n",
    "                print(\"Image: {0}, loss: {1}\".format(num, loss.item()))\n",
    "                \n",
    "                \n",
    "                save_original = 'Orig_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                    .format(epoch, lr, weight_decay, num)\n",
    "                save_new = 'new_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                    .format(epoch, lr, weight_decay, num)\n",
    "                \n",
    "                to_rgb(channel_l[0].cpu(), channel_a_b[0].cpu(),\n",
    "                    save_path = save_path, save_name = save_original, device = device)\n",
    "                to_rgb(channel_l[0].cpu(), channel_a_b[0].cpu(),\n",
    "                    save_path = save_path, save_name = save_new, device = device)\n",
    "                \n",
    "        self.display_imgs(epoch, lr, weight_decay, save_path)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def display_imgs(epoch, lr, weight_decay, save_path):\n",
    "        color_path = save_path['colorized']\n",
    "        gray_path = save_path['grayscale']\n",
    "            \n",
    "            \n",
    "        for i in range(7,70,7):\n",
    "            title = \"\".\\\n",
    "                format(epoch, lr, weight_decay, i)\n",
    "            save_original = 'orig_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                   .format(epoch, lr, weight_decay, i)\n",
    "            save_new = 'new_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                    .format(epoch, lr, weight_decay, i)\n",
    "            display_image(gray_path + save_original, color_path + save_original, \n",
    "                              color_path + save_new, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Myregressor = Reg()\n",
    "# Myregressor.reg_train(augmented_batch_train,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Myregressor.reg_test(augmented_batch_train,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Activation Function... relu\n",
      "...colorizer training started...\n",
      "Epoch: 100, lr: 0.001, weight decay:1e-05\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_gray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e0e55307cd1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/aashish/Desktop/DLCG2/Graphs/Colorizer/Colorizer_sigmoid_epoch{0}_lr_{1}_weight_decay_{2}.pth\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mMyregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_colorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-d19a67bd0121>\u001b[0m in \u001b[0;36mtrain_colorizer\u001b[1;34m(self, augmented_dataset_batch, activation_function, model_name, device)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maugmented_dataset_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m                     \u001b[0mchannel_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[0mchannel_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchannel_l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0d087916e44d>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mchannel_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mimg_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mchannel_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_gray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_gray' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = \"C:/Users/aashish/Desktop/DLCG2/Graphs/Colorizer/Colorizer_sigmoid_epoch{0}_lr_{1}_weight_decay_{2}.pth\"\n",
    "Myregressor.train_colorizer(augmented_batch_train, \"relu\", model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = {'grayscale': 'C:/Users/aashish/Desktop/DLCG2/Graphs/Colorizer/outputs/gray1/', 'colorized': 'C:/Users/aashish/Desktop/DLCG2/Graphs/Colorizer/outputs/color1/'}\n",
    "\n",
    "Myregressor.test_colorizer(augmented_batch_test, \"relu\", save_path, model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_rgb(image):\n",
    "#     scale = random.uniform(0.6,1)\n",
    "#     scaled_rgb_image = image*scale\n",
    "#     return scaled_rgb_image\n",
    "\n",
    "# def flip():a\n",
    "#     return random.choice([True,False])\n",
    "\n",
    "# def augment_dataset(images, n):\n",
    "#     num_images = images.shape[0]\n",
    "#     augmented_data = torch.empty((n*num_images)+num_images, 3, 128, 128)\n",
    "    \n",
    "#     #random cropping\n",
    "#     crop = transforms.Compose([transforms.RandomResizedCrop(128,128)])\n",
    "#     horizontal_flip = transforms.Compose([transforms.RandomHorizontalFlip(p=.7)])\n",
    "    \n",
    "#     num = 0\n",
    "    \n",
    "#     #original training set\n",
    "#     for i in images:\n",
    "#         augmented_data[num] = torch.Tensor(np.array(image).astype(np.uint8))\n",
    "#         num+=1\n",
    "        \n",
    "#     for i in range(n):\n",
    "#         for image in images:\n",
    "#             if flip:\n",
    "#                 transformed_image = crop(image)\n",
    "#             transformed_image = horizontal_flip(image)\n",
    "#             transformed_image = scale_rgb(transformed_image)\n",
    "#             augmented_data[num] = torch.Tensor(np.array(transformed_image).astype(np.uint8))\n",
    "#             num+=1\n",
    "#     return augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 : convert to LAB color space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_LAB(images):\n",
    "#     num_images = images.shape[0]\n",
    "#     LAB_data = torch.empty(num_images,3,128,128)\n",
    "    \n",
    "#     images = images.permute(num_images, 3, 128, 128)\n",
    "#     for image in enumerate(images):\n",
    "#         image = np.array(image).astype(np.uint8)\n",
    "#         imageLAB = cv2.cvtColor(image, COLOR_RGB2LAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class model():\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = nn.Sequential(nn.Conv2d(1, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer2 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer3 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer4 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer5 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer6 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer7 = nn.Linear(2*2*3,2)\n",
    "#     def forward(self, X):\n",
    "#         X = self.layer1(X)\n",
    "#         X = self.layer2(X)\n",
    "#         X = self.layer3(X)\n",
    "#         X = self.layer4(X)\n",
    "#         X = self.layer5(X)\n",
    "#         X = self.layer6(X)\n",
    "#         tensor.reshape(X, (12,1))\n",
    "#         X = self.layer7(X)\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABdata = []\n",
    "# for i in data:\n",
    "#     imageLAB = cv.cvtColor(i, cv.COLOR_BGR2LAB)\n",
    "#     LABdata.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
