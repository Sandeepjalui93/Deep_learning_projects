{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from shutil import copy2\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "import re\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting the default data type to float 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 : Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of given list: 750\n",
      "number of images: 750\n",
      "Training Set Size: 675\n",
      "Testing Set Size: 75\n",
      "675\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"C:/Users/aashish/Desktop/DLCG2/face_images/\"\n",
    "files = glob.glob(\"C:/Users/aashish/Desktop/DLCG2/face_images/*.jpg\")\n",
    "\n",
    "\n",
    "print(\"Length of given list:\", len(files))\n",
    "train_data = \"C:/Users/aashish/Desktop/DLCG2/train/tensor/\"\n",
    "test_data = \"C:/Users/aashish/Desktop/DLCG2/test/tensor/\"\n",
    "\n",
    "os.makedirs(train_data, exist_ok = True)\n",
    "os.makedirs(test_data, exist_ok = True)\n",
    "\n",
    "num_images = len(next(os.walk(img_dir))[2])\n",
    "print(\"number of images:\", num_images)\n",
    "\n",
    "for i, file in enumerate(os.listdir(img_dir)):\n",
    "    if i < (0.1*num_images):\n",
    "        copy2(img_dir + file, test_data + file)\n",
    "        continue\n",
    "    else:\n",
    "        copy2(img_dir + file, train_data + file)\n",
    "\n",
    "print(\"Training Set Size:\", len(next(os.walk(train_data))[2]))\n",
    "print(\"Testing Set Size:\", len(next(os.walk(test_data))[2]))\n",
    "\n",
    "\n",
    "training_data = glob.glob('C:/Users/aashish/Desktop/DLCG2/train/tensor/*.jpg')\n",
    "testing_data = glob.glob('C:/Users/aashish/Desktop/DLCG2/test/tensor/*.jpg')\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "# # for f1 in files:\n",
    "# #     img = cv.imread(f1)\n",
    "# #     img = cv.cvtColor(img, cv.COLOR_BGR2RGB) #converting to RGB\n",
    "# #     data.append(img)\n",
    "# # data_tensor = torch.tensor(data).permute(0,3,1,2) #Converting nparray to tensor and in desired order\n",
    "# # print (img.shape)\n",
    "# print (data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class to return augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedImageDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self,index):\n",
    "        global channel_a, channel_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        original_image = np.asarray(img)\n",
    "        \n",
    "        img_lab = rgb2lab(original_image)\n",
    "        img_lab = img_lab + 128\n",
    "        img_lab = img_lab / 255\n",
    "        \n",
    "        channel_a = img_lab[:, :, 1:2]\n",
    "        channel_a = torch.from_numpy(channel_a.transpose((2,0,1))).float()\n",
    "        \n",
    "        channel_b = img_lab[:, :, 2:3]\n",
    "        channel_b = torch.from_numpy(channel_b.transpose((2,0,1))).float()\n",
    "        \n",
    "        img_gray = rgb2gray(original_image)\n",
    "        img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
    "        \n",
    "        return channel_a, channel_b, img_gray\n",
    "    \n",
    "    \n",
    "class AugmentedImageDataset_RELU(datasets.ImageFolder):\n",
    "    def __getitem__(self,index):\n",
    "        global channel_a, channel_b, img_gray\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self. transform(img)\n",
    "        original_image = np.asarray(img)\n",
    "        \n",
    "        img_lab = rgb2lab(original_image)\n",
    "        channel_a = img_lab[:, :, 1:2]\n",
    "        channel_a = torch.from_numpy(channel_a.transpose((2,0,1))).float()\n",
    "        \n",
    "        channel_b = img_lab[:, :, 2:3]\n",
    "        channel_b = torch.from_numpy(channel_b.transpose((2,0,1))).float()\n",
    "        \n",
    "        img_gray = rgb2gray(original_image)\n",
    "        img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
    "        \n",
    "        return channel_a, channel_b, img_gray\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### augmenting the dataset and loading into 10* tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(128),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.RandomResizedCrop(128),\n",
    "#                                 transforms.ToTensor(), \n",
    "#                                        transforms.Normalize([0.6, 0.6, 0.6], [0.6, 0.6, 0.6])\n",
    "                               ])\n",
    "\n",
    "train_dataset =[]\n",
    "train_dataset.append(AugmentedImageDataset_RELU('C:/Users/aashish/Desktop/DLCG2/train'))\n",
    "for i in range(9):\n",
    "    train_dataset.append(AugmentedImageDataset_RELU('C:/Users/aashish/Desktop/DLCG2/train', transform))\n",
    "    \n",
    "augmented_dataset = ConcatDataset(train_dataset)\n",
    "print(len(augmented_dataset))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading all the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = dict(shuffle = True, batch_size = 32)\n",
    "# if cuda:\n",
    "#     train_args = dict(shuffle = True, batch_size = 8, num_workers = 1, pin_memory = True)\n",
    "    \n",
    "augmented_batch_train = DataLoader(dataset = augmented_dataset, **train_args)\n",
    "augmented_batch_test = DataLoader(dataset = AugmentedImageDataset('C:/Users/aashish/Desktop/DLCG2/test')) \n",
    "#not applying transforms on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available()else  \"cpu\")\n",
    "is_cuda_available = True if torch.cuda.is_available() else False\n",
    "if is_cuda_available:\n",
    "    num_workers = 8\n",
    "else:\n",
    "    num_workers = 0\n",
    "# print(device)\n",
    "print(format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel=1, hidden_channel=3, out_dims=2, train_mode = \"regressor\"):\n",
    "        super(Regressor,self).__init__()\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channel, out_channels = 32, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels =128, out_channels = 256, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size=2, stride=2,padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        if self.train_mode == \"regressor\":\n",
    "            self.lin = nn.Linear(in_features=512 * 2 * 2, out_features=out_dims)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        if self.train_mode == \"regressor\":\n",
    "            y = torch.sigmoid(self.lin(features.reshape(-1,512*2*2)))\n",
    "            return y\n",
    "        else:\n",
    "            return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer(nn.Module):\n",
    "    def __init__(self, in_channel=3, hidden_channel=3, out_channel=2,activation_function = \"sigmoid\"):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        self.features = Regressor(in_channel=1, hidden_channel = 3, out_dims=2, train_mode=\"colorizer\")\n",
    "        \n",
    "        self.up_sampling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels = 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels = 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels = 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels = 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels = 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels = out_channel, kernel_size=4, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            return torch.sigmoid(self.up_sampling(self.features(x)))\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return torch.tanh(self.up_sampling(self.features(x)))\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return torch.relu(self.up_sampling(self.features(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ab_mean(a_channel, b_channel):\n",
    "    a_channel_mean = a_channel.mean(dim = (2,3))\n",
    "    b_channel_mean = b_channel.mean(dim=(2,3))\n",
    "    a_b_mean = torch.cat([a_channel_mean,b_channel_mean], dim = 1)\n",
    "    \n",
    "    return a_b_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(gray_input, ab_input, save_path = None, save_name = None, device = \"cpu\"):\n",
    "    plt.clf()\n",
    "    color_image = torch.cat((gray_input, ab_input), 0).numpy()\n",
    "    color_image = color_image.transpose((1,2,0))\n",
    "    \n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
    "    color_image = lab2rgb(color_image.astype(np.float64))\n",
    "    gray_input = gray_input.squeeze().numpy()\n",
    "    if save_path is not None and save_name is not None:\n",
    "        plt.imsave(arr=gray_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap = 'gray')\n",
    "        plt.imsave(arr=color_image, fname = '{}{}'.format(save_path['colorized'], save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(gray, orig, new, fig_name):\n",
    "    plt.clf()\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1, 3, 1)\n",
    "    plt.imshow(mpimg.imread(gray))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 2)\n",
    "    plt.imshow(mpimg.imread(orig))\n",
    "    plt.axis('off')\n",
    "    f.add_subplot(1, 3, 3)\n",
    "    plt.imshow(mpimg.imread(new))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.draw()\n",
    "    plt.savefig(fig_name, dpi=220)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping_DCN:\n",
    "    def __init__(self, patience = 7, verbose = False, delta = 0, model_path = None, trace_func = print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.model_path = model_path\n",
    "        self.trace_func = trace_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hyperparameters():\n",
    "    parameters = dict(lr=[0.001],\n",
    "                     weight_decay=[1e-5],\n",
    "                     epoch=[100])\n",
    "    hyperparam = [i for i in parameters.values()]\n",
    "    return hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg:\n",
    "    def reg_train(self, augmented_dataset_batch, device):\n",
    "        print(\"...Regressor training Started...\")\n",
    "        model = Regressor(in_channel = 1, hidden_channel = 3, out_dims = 2, train_mode = \"regressor\").to(device)\n",
    "\n",
    "        lossF = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-4)\n",
    "\n",
    "        loss_train = []\n",
    "\n",
    "        #start training\n",
    "        epochs = 100\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "\n",
    "            for batch in augmented_dataset_batch:\n",
    "                l_channel, a_channel, b_channel = batch\n",
    "                l_channel = l_channel.to(device)\n",
    "\n",
    "                a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "                a_b_mean_hat = model(l_channel)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    loss = lossF(a_b_mean_hat.float().cuda(),a_b_mean.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss = Lossf(a_b_mean_hat.float(), a_b_mean.float()).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"epoch: {0}, loss: {1}\".format(epoch, total_loss))\n",
    "            loss_train.append(total_loss)\n",
    "\n",
    "        # plotting loss/ epoch graph\n",
    "        plt.ion()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(loss_train)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        plt.draw()\n",
    "        plt.savefig('C:/Users/aashish/Desktop/DLCG2/Graphs', dpi = 200)\n",
    "        plt.clf()\n",
    "        torch.save(model.state_dict(), \"C:/Users/aashish/Desktop/DLCG2/Graphs/Reg.pth\")\n",
    "        \n",
    "    def reg_test(self, augmented_dataset_batch, device):\n",
    "        print(\"<<Regressor testing started>>\")\n",
    "        model = Regressor(in_channel= 1, hidden_channel=3, out_dims = 2, train_mode = \"regressor\").to(device)\n",
    "        model.load_state_dict(torch.load(\"C:/Users/aashish/Desktop/DLCG2/Graphs/Reg.pth\", map_location=device))\n",
    "        \n",
    "        a_list = []\n",
    "        b_list = []\n",
    "        lossF = nn.MSELoss()\n",
    "        total_loss = 0\n",
    "        loss_test = []\n",
    "        for batch in augmented_dataset_batch:\n",
    "            l_channel, a_channel, b_channel = batch\n",
    "            l_channel = l_channel.to(device)\n",
    "            \n",
    "            a_b_mean = get_ab_mean(a_channel, b_channel)\n",
    "            a_b_mean_hat = model(l_channel).detach()\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                loss = lossF(a_b_mean_hat.float().cuda(), a_b_mean.float().cuda()).to(device)\n",
    "            else:\n",
    "                loss = lossF(a_b_mean_hat.float(), a_b_mean.float()).to(device)\n",
    "            loss_test.append(loss.item())\n",
    "            \n",
    "            a_b_pred = a_b_mean_hat[0].cpu().numpy()\n",
    "            a_list.append(a_b_pred[0])\n",
    "            b_list.append(a_b_pred[1])\n",
    "        print(\"MSE:\", np.average(np.asarray(loss_test)))\n",
    "        print(\"Num_Image || Mean a || Mean b\")\n",
    "        for i in range(1, len(a_list)):\n",
    "            print(\"Image:{0} mean_a: {1} mean_b:{2}\".format(i, (a_list[i] * 255)-128, (b_list[i]*255)-128))\n",
    "            \n",
    "    def train_colorizer(self, augmented_dataset_batch, activation_function, model_name, device):\n",
    "        print(\"...Activation Function...\", activation_function)\n",
    "        print(\"...colorizer training started...\")\n",
    "        \n",
    "        \n",
    "        parameters = Hyperparameters()\n",
    "        for lr, weight_decay, epoch in product(*parameters):\n",
    "            print(\"Epoch: {0}, lr: {1}, weight decay:{2}\".format(epoch, lr, weight_decay))\n",
    "            model = Colorizer(in_channel=3, hidden_channel=3, activation_function = activation_function).to(device)\n",
    "        \n",
    "            lossF = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr =lr, weight_decay = weight_decay)\n",
    "#             saved_model_path = model_name.format(epoch, lr, weight_decay)\n",
    "            loss_train = []\n",
    "            early_stopping = EarlyStopping_DCN(patience = 50, verbose=True, model_path=\"C:/Users/aashish/Desktop/DLCG2/Graphs/color.pth\")\n",
    "#             epochs = 100\n",
    "            for epoch in range(epoch):\n",
    "                total_train_loss = 0\n",
    "                total_val_loss = 0\n",
    "                model.train()\n",
    "                \n",
    "                for batch in augmented_dataset_batch:\n",
    "                    channel_l, channel_a, channel_b = batch\n",
    "                    channel_l = channel_l.to(device)\n",
    "                    \n",
    "                    channel_a_b = torch.cat([channel_a, channel_b], dim = 1)\n",
    "                    channel_a_b_hat = model(channel_l)\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        loss = lossF(channel_a_b_hat.float().cuda(), channel_a_b.float().cuda()).to(device)\n",
    "                    else:\n",
    "                        loss = lossF(channel_a_b_hat.float(), channel_a_b.float()).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_train_loss += loss.item()\n",
    "                    \n",
    "                print(\"epoch:{0}, loss:{1}\".format(epoch, total_train_loss))\n",
    "                loss_train.append(total_train_loss)\n",
    "                \n",
    "                \n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early Stop\")\n",
    "                    break\n",
    "                    \n",
    "                # plotting loss/ epoch graph\n",
    "            plt.ion()\n",
    "            fig = plt.figure()\n",
    "            plt.plot(loss_train)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.show()\n",
    "            plt.draw()\n",
    "            plt.savefig('C:/Users/aashish/Desktop/DLCG2/Graphs/loss_plot_path.jpeg', dpi = 220)\n",
    "            plt.clf()\n",
    "            torch.save(model.state_dict(), \"C:/Users/aashish/Desktop/DLCG2/Graphs/color.pth\")\n",
    "        \n",
    "        \n",
    "    def test_colorizer(self, augmented_dataset_batch, activation_function, save_path, model_name, device):\n",
    "        parameters = Hyperparameters()\n",
    "        for lr, weight_decay, epoch in product(*parameters):\n",
    "#             print(\"------\")\n",
    "            print(\"Epoch : {0}, lr: {1}, Weight_decay: {2}\".format(epoch, lr, weight_decay))\n",
    "            \n",
    "#             saved_model_path = model_name.format(epoch, lr, weight_decay)\n",
    "                \n",
    "            print(activation_function)\n",
    "            print(\"--- Colorizer Testing Started ---\")\n",
    "            model = Colorizer(in_channel=3, hidden_channel = 3, activation_function = activation_function).to(device)\n",
    "            model.load_state_dict(torch.load(\"C:/Users/aashish/Desktop/DLCG2/Graphs/color.pth\", map_location = device))\n",
    "                \n",
    "            lossF = nn.MSELoss()\n",
    "            num = 0\n",
    "            for batch in augmented_dataset_batch:\n",
    "                num += 1\n",
    "                channel_l, channel_a, channel_b = batch\n",
    "                channel_l = channel_l.to(device)\n",
    "                    \n",
    "                channel_a_b = torch.cat([channel_a, channel_b], dim=1)\n",
    "                channel_a_b_hat = model(channel_l).detach()\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    loss = lossF(channel_a_b_hat.float().cuda(), channel_a_b.float().cuda()).to(device)\n",
    "                else:\n",
    "                    loss = lossF(channel_a_b_hat.float(), channel_a_b.float()).to(device)\n",
    "                    \n",
    "                print(\"Image: {0}, loss: {1}\".format(num, loss.item()))\n",
    "                \n",
    "                \n",
    "                save_original = 'Orig_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                    .format(epoch, lr, weight_decay, num)\n",
    "                save_new = 'new_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                    .format(epoch, lr, weight_decay, num)\n",
    "                \n",
    "                to_rgb(channel_l[0].cpu(), channel_a_b[0].cpu(),\n",
    "                    save_path = save_path, save_name = save_original, device = device)\n",
    "                to_rgb(channel_l[0].cpu(), channel_a_b[0].cpu(),\n",
    "                    save_path = save_path, save_name = save_new, device = device)\n",
    "                \n",
    "        self.display_imgs(epoch, lr, weight_decay, save_path)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def display_imgs(epoch, lr, weight_decay, save_path):\n",
    "        color_path = save_path['colorized']\n",
    "        gray_path = save_path['grayscale']\n",
    "            \n",
    "            \n",
    "        for i in range(7,70,7):\n",
    "            title = \"\".\\\n",
    "                format(epoch, lr, weight_decay, i)\n",
    "            save_original = 'orig_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                   .format(epoch, lr, weight_decay, i)\n",
    "            save_new = 'new_img_epoch_{0}_lr_{1}_wt_decay_{2}_num_{3}.jpg' \\\n",
    "                    .format(epoch, lr, weight_decay, i)\n",
    "            display_image(gray_path + save_original, color_path + save_original, \n",
    "                              color_path + save_new, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Myregressor = Reg()\n",
    "# Myregressor.reg_train(augmented_batch_train,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Myregressor.reg_test(augmented_batch_train,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Activation Function... relu\n",
      "...colorizer training started...\n",
      "Epoch: 100, lr: 0.001, weight decay:1e-05\n",
      "epoch:0, loss:8166.41140460968\n",
      "epoch:1, loss:2888.1828570365906\n",
      "epoch:2, loss:2640.090546131134\n",
      "epoch:3, loss:2374.356083869934\n",
      "epoch:4, loss:2227.2976174354553\n",
      "epoch:5, loss:2147.0645837783813\n",
      "epoch:6, loss:2057.22362947464\n",
      "epoch:7, loss:1966.3329672813416\n",
      "epoch:8, loss:1878.1482996940613\n",
      "epoch:9, loss:1841.0754404067993\n",
      "epoch:10, loss:1755.4487648010254\n",
      "epoch:11, loss:1739.301275253296\n",
      "epoch:12, loss:1697.2495679855347\n",
      "epoch:13, loss:1641.2599992752075\n",
      "epoch:14, loss:1605.9400792121887\n",
      "epoch:15, loss:1602.3981761932373\n",
      "epoch:16, loss:1539.2620344161987\n",
      "epoch:17, loss:1499.8283061981201\n",
      "epoch:18, loss:1534.5791788101196\n",
      "epoch:19, loss:1464.440800189972\n",
      "epoch:20, loss:1451.191620349884\n",
      "epoch:21, loss:1395.0835285186768\n",
      "epoch:22, loss:1380.428141117096\n",
      "epoch:23, loss:1344.849193096161\n",
      "epoch:24, loss:1393.6881093978882\n",
      "epoch:25, loss:1343.4942598342896\n",
      "epoch:26, loss:1370.093454837799\n",
      "epoch:27, loss:1295.668538570404\n",
      "epoch:28, loss:1314.3366589546204\n",
      "epoch:29, loss:1279.765387058258\n",
      "epoch:30, loss:1244.1843008995056\n",
      "epoch:31, loss:1254.98823595047\n",
      "epoch:32, loss:1279.0800485610962\n",
      "epoch:33, loss:1195.2397420406342\n",
      "epoch:34, loss:1244.156958580017\n",
      "epoch:35, loss:1205.5568399429321\n",
      "epoch:36, loss:1205.3432772159576\n",
      "epoch:37, loss:1135.5228157043457\n",
      "epoch:38, loss:1142.3037128448486\n",
      "epoch:39, loss:1150.485009431839\n",
      "epoch:40, loss:1150.927904844284\n",
      "epoch:41, loss:1089.3577075004578\n",
      "epoch:42, loss:1127.763221025467\n",
      "epoch:43, loss:1119.0371742248535\n",
      "epoch:44, loss:1088.9796357154846\n",
      "epoch:45, loss:1057.0243980884552\n",
      "epoch:46, loss:1081.7766082286835\n",
      "epoch:47, loss:1041.7941272258759\n",
      "epoch:48, loss:1055.9972352981567\n",
      "epoch:49, loss:1002.9740841388702\n",
      "epoch:50, loss:988.440262556076\n",
      "epoch:51, loss:1004.175375699997\n",
      "epoch:52, loss:1088.869688987732\n",
      "epoch:53, loss:1007.5342283248901\n",
      "epoch:54, loss:993.4441661834717\n",
      "epoch:55, loss:980.878769159317\n",
      "epoch:56, loss:967.4308059215546\n",
      "epoch:57, loss:969.9826242923737\n",
      "epoch:58, loss:936.0611271858215\n",
      "epoch:59, loss:949.3138318061829\n",
      "epoch:60, loss:952.5357944965363\n",
      "epoch:61, loss:924.3578736782074\n",
      "epoch:62, loss:983.1963024139404\n",
      "epoch:63, loss:889.6038699150085\n",
      "epoch:64, loss:927.3276834487915\n",
      "epoch:65, loss:929.1567852497101\n",
      "epoch:66, loss:885.496593952179\n",
      "epoch:67, loss:887.7408134937286\n",
      "epoch:68, loss:902.1520175933838\n",
      "epoch:69, loss:937.1589207649231\n",
      "epoch:70, loss:918.4358913898468\n",
      "epoch:71, loss:884.9375336170197\n",
      "epoch:72, loss:840.8998169898987\n",
      "epoch:73, loss:870.481493473053\n",
      "epoch:74, loss:863.2428712844849\n",
      "epoch:75, loss:864.0565464496613\n",
      "epoch:76, loss:865.0158922672272\n",
      "epoch:77, loss:830.2532525062561\n",
      "epoch:78, loss:889.5974485874176\n",
      "epoch:79, loss:827.5041100978851\n",
      "epoch:80, loss:799.4001281261444\n",
      "epoch:81, loss:862.8964540958405\n",
      "epoch:82, loss:846.9904181957245\n",
      "epoch:83, loss:853.7468400001526\n",
      "epoch:84, loss:802.8620855808258\n",
      "epoch:85, loss:835.3965590000153\n",
      "epoch:86, loss:824.2003121376038\n",
      "epoch:87, loss:832.6974110603333\n",
      "epoch:88, loss:819.4025356769562\n",
      "epoch:89, loss:792.9635870456696\n",
      "epoch:90, loss:775.9168980121613\n",
      "epoch:91, loss:774.0974445343018\n",
      "epoch:92, loss:760.0848090648651\n",
      "epoch:93, loss:780.4079864025116\n",
      "epoch:94, loss:768.542010307312\n",
      "epoch:95, loss:804.7710809707642\n",
      "epoch:96, loss:806.3116505146027\n",
      "epoch:97, loss:730.2266607284546\n",
      "epoch:98, loss:771.962560415268\n",
      "epoch:99, loss:779.5937798023224\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"C:/Users/aashish/Desktop/DLCG2/Graphs/Colorizer/Colorizer_sigmoid_epoch{0}_lr_{1}_weight_decay_{2}.pth\"\n",
    "Myregressor.train_colorizer(augmented_batch_train, \"relu\", model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100, lr: 0.001, Weight_decay: 1e-05\n",
      "relu\n",
      "--- Colorizer Testing Started ---\n",
      "Image: 1, loss: 55.76398468017578\n",
      "Image: 2, loss: 54.529869079589844\n",
      "Image: 3, loss: 62.91149139404297\n",
      "Image: 4, loss: 58.13284683227539\n",
      "Image: 5, loss: 73.75316619873047\n",
      "Image: 6, loss: 58.52783966064453\n",
      "Image: 7, loss: 64.69279479980469\n",
      "Image: 8, loss: 59.199134826660156\n",
      "Image: 9, loss: 51.15878677368164\n",
      "Image: 10, loss: 52.1242790222168\n",
      "Image: 11, loss: 67.8384017944336\n",
      "Image: 12, loss: 49.79917526245117\n",
      "Image: 13, loss: 52.95988082885742\n",
      "Image: 14, loss: 59.107303619384766\n",
      "Image: 15, loss: 65.182373046875\n",
      "Image: 16, loss: 57.53767013549805\n",
      "Image: 17, loss: 64.28279113769531\n",
      "Image: 18, loss: 59.499732971191406\n",
      "Image: 19, loss: 61.998573303222656\n",
      "Image: 20, loss: 61.11003494262695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashish\\anaconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:1128: UserWarning: Color data out of range: Z < 0 in 2 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 21, loss: 58.49220657348633\n",
      "Image: 22, loss: 72.19178009033203\n",
      "Image: 23, loss: 61.091793060302734\n",
      "Image: 24, loss: 64.56071472167969\n",
      "Image: 25, loss: 49.248348236083984\n",
      "Image: 26, loss: 67.35834503173828\n",
      "Image: 27, loss: 69.68585968017578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashish\\anaconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:1128: UserWarning: Color data out of range: Z < 0 in 4 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 28, loss: 54.6670036315918\n",
      "Image: 29, loss: 50.2679443359375\n",
      "Image: 30, loss: 49.76628494262695\n",
      "Image: 31, loss: 52.64501953125\n",
      "Image: 32, loss: 53.41197204589844\n",
      "Image: 33, loss: 47.950531005859375\n",
      "Image: 34, loss: 54.520172119140625\n",
      "Image: 35, loss: 55.85875701904297\n",
      "Image: 36, loss: 60.26262664794922\n",
      "Image: 37, loss: 50.413814544677734\n",
      "Image: 38, loss: 67.6167221069336\n",
      "Image: 39, loss: 54.545711517333984\n",
      "Image: 40, loss: 45.010799407958984\n",
      "Image: 41, loss: 76.28506469726562\n",
      "Image: 42, loss: 49.796234130859375\n",
      "Image: 43, loss: 48.09047317504883\n",
      "Image: 44, loss: 63.179054260253906\n",
      "Image: 45, loss: 54.20442199707031\n",
      "Image: 46, loss: 62.671546936035156\n",
      "Image: 47, loss: 51.80669403076172\n",
      "Image: 48, loss: 59.134708404541016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashish\\anaconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:1128: UserWarning: Color data out of range: Z < 0 in 3 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 49, loss: 92.39384460449219\n",
      "Image: 50, loss: 47.31529235839844\n",
      "Image: 51, loss: 67.7257080078125\n",
      "Image: 52, loss: 51.331966400146484\n",
      "Image: 53, loss: 51.8331413269043\n",
      "Image: 54, loss: 57.34105682373047\n",
      "Image: 55, loss: 57.3055305480957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashish\\anaconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:1128: UserWarning: Color data out of range: Z < 0 in 1 pixels\n",
      "  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 56, loss: 73.72955322265625\n",
      "Image: 57, loss: 51.68040084838867\n",
      "Image: 58, loss: 60.17959213256836\n",
      "Image: 59, loss: 57.83360290527344\n",
      "Image: 60, loss: 60.95484924316406\n",
      "Image: 61, loss: 65.03648376464844\n",
      "Image: 62, loss: 60.930362701416016\n",
      "Image: 63, loss: 60.006134033203125\n",
      "Image: 64, loss: 51.10940933227539\n",
      "Image: 65, loss: 52.28996276855469\n",
      "Image: 66, loss: 61.08475875854492\n",
      "Image: 67, loss: 53.85777282714844\n",
      "Image: 68, loss: 58.469940185546875\n",
      "Image: 69, loss: 47.7750129699707\n",
      "Image: 70, loss: 58.014495849609375\n",
      "Image: 71, loss: 66.34459686279297\n",
      "Image: 72, loss: 61.715126037597656\n",
      "Image: 73, loss: 48.22356414794922\n",
      "Image: 74, loss: 53.2819938659668\n",
      "Image: 75, loss: 72.52864074707031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_path = {'grayscale': 'C:/Users/aashish/Desktop/DLCG2/Graphs/Colorizer/outputs/gray1/', 'colorized': 'C:/Users/aashish/Desktop/DLCG2/Graphs/Colorizer/outputs/color1/'}\n",
    "\n",
    "Myregressor.test_colorizer(augmented_batch_test, \"relu\", save_path, model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_rgb(image):\n",
    "#     scale = random.uniform(0.6,1)\n",
    "#     scaled_rgb_image = image*scale\n",
    "#     return scaled_rgb_image\n",
    "\n",
    "# def flip():a\n",
    "#     return random.choice([True,False])\n",
    "\n",
    "# def augment_dataset(images, n):\n",
    "#     num_images = images.shape[0]\n",
    "#     augmented_data = torch.empty((n*num_images)+num_images, 3, 128, 128)\n",
    "    \n",
    "#     #random cropping\n",
    "#     crop = transforms.Compose([transforms.RandomResizedCrop(128,128)])\n",
    "#     horizontal_flip = transforms.Compose([transforms.RandomHorizontalFlip(p=.7)])\n",
    "    \n",
    "#     num = 0\n",
    "    \n",
    "#     #original training set\n",
    "#     for i in images:\n",
    "#         augmented_data[num] = torch.Tensor(np.array(image).astype(np.uint8))\n",
    "#         num+=1\n",
    "        \n",
    "#     for i in range(n):\n",
    "#         for image in images:\n",
    "#             if flip:\n",
    "#                 transformed_image = crop(image)\n",
    "#             transformed_image = horizontal_flip(image)\n",
    "#             transformed_image = scale_rgb(transformed_image)\n",
    "#             augmented_data[num] = torch.Tensor(np.array(transformed_image).astype(np.uint8))\n",
    "#             num+=1\n",
    "#     return augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 : convert to LAB color space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_LAB(images):\n",
    "#     num_images = images.shape[0]\n",
    "#     LAB_data = torch.empty(num_images,3,128,128)\n",
    "    \n",
    "#     images = images.permute(num_images, 3, 128, 128)\n",
    "#     for image in enumerate(images):\n",
    "#         image = np.array(image).astype(np.uint8)\n",
    "#         imageLAB = cv2.cvtColor(image, COLOR_RGB2LAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class model():\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = nn.Sequential(nn.Conv2d(1, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer2 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer3 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer4 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer5 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer6 = nn.Sequential(nn.Conv2d(3, 3, kernel_size=2, stride=2, padding=0), nn.ReLu())\n",
    "#         self.layer7 = nn.Linear(2*2*3,2)\n",
    "#     def forward(self, X):\n",
    "#         X = self.layer1(X)\n",
    "#         X = self.layer2(X)\n",
    "#         X = self.layer3(X)\n",
    "#         X = self.layer4(X)\n",
    "#         X = self.layer5(X)\n",
    "#         X = self.layer6(X)\n",
    "#         tensor.reshape(X, (12,1))\n",
    "#         X = self.layer7(X)\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABdata = []\n",
    "# for i in data:\n",
    "#     imageLAB = cv.cvtColor(i, cv.COLOR_BGR2LAB)\n",
    "#     LABdata.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
